"""MMSI-Bench dataset utilities."""

import json
import re
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List

import pandas as pd


def load_mmsibench_dataset(data_file: Path, limit: int = None) -> List[Dict[str, Any]]:
    """Load MMSI-Bench dataset from TSV file.
    
    Args:
        data_file: Path to the TSV file containing MMSI-Bench data
        limit: Optional limit on number of samples to load
        
    Returns:
        List of dataset items, each containing:
            - index: Unique identifier
            - image: List of image paths (parsed from JSON array)
            - question: Question text with options
            - answer: Correct answer (A/B/C/D)
            - category: Task category (11 types)
            - thought: Step-by-step reasoning
            - difficulty: Difficulty level
    """
    # Load TSV file
    df = pd.read_csv(data_file, sep='\t')
    
    if limit is not None:
        df = df.head(limit)
    
    dataset = []
    for idx, row in df.iterrows():
        # Parse image field (JSON array of paths)
        image_field = row.get('image', '[]')
        if isinstance(image_field, str):
            try:
                image_paths = json.loads(image_field)
            except json.JSONDecodeError:
                # If not JSON, treat as single path
                image_paths = [image_field]
        else:
            image_paths = [image_field]
        
        item = {
            'index': row.get('index', idx),
            'image': image_paths,  # List of image paths
            'question': row.get('question', ''),
            'answer': row.get('answer', ''),
            'category': row.get('category', 'unknown'),
            'thought': row.get('thought', ''),
            'difficulty': row.get('difficulty', 'unknown'),
        }
        dataset.append(item)
    
    return dataset


def load_mmsibench_dataset_json(data_file: Path, limit: int = None) -> List[Dict[str, Any]]:
    """Load MMSI-Bench dataset from the pre-processed test_data_final.json file.

    The JSON file is generated by download.py and stores decoded images as local
    files. Each entry has the following keys:
        - id: sample index (int)
        - question: question text with options
        - answer: correct answer letter (A/B/C/D)
        - thought_gt: ground-truth reasoning chain
        - type: question category string (e.g. "Motion (Cam.)")
        - local_images: list of relative image paths (relative to the JSON's
          parent directory, e.g. "./data/images/sample_0_img_0.jpg")

    Args:
        data_file: Path to test_data_final.json
        limit: Optional limit on number of samples to load

    Returns:
        List of dataset items with unified keys:
            - index: Unique identifier (from 'id')
            - image: List of resolved image path strings (from 'local_images')
            - question: Question text with options
            - answer: Correct answer (A/B/C/D)
            - category: Task category (from 'type')
            - thought: Step-by-step reasoning (from 'thought_gt')
            - difficulty: 'unknown' (field not present in JSON)
    """
    with open(data_file, "r", encoding="utf-8") as f:
        raw = json.load(f)

    if limit is not None:
        raw = raw[:limit]

    # Image paths in the JSON are stored relative to the MMSIBench root
    # (the directory that contains the 'data/' sub-folder), NOT relative to
    # the JSON file itself.  e.g. "./data/images/sample_0_img_0.jpg" was
    # written when download.py ran from MMSIBench/.
    # The JSON lives at MMSIBench/data/test_data_final.json, so:
    #   json_dir               → MMSIBench/data/
    #   json_dir.parent (root) → MMSIBench/
    mmsibench_root = data_file.parent.parent

    dataset = []
    for item in raw:
        local_images = item.get("local_images", [])
        # Resolve each path relative to the JSON directory
        image_paths = []
        for p in local_images:
            resolved = (mmsibench_root / p).resolve()
            image_paths.append(str(resolved))

        dataset.append({
            "index": item.get("id", ""),
            "image": image_paths,
            "question": item.get("question", ""),
            "answer": item.get("answer", ""),
            "category": item.get("type", "unknown"),
            "thought": item.get("thought_gt", ""),
            "difficulty": "unknown",
        })

    return dataset


def extract_answer_letter(text: str) -> str:
    """Extract answer letter (A/B/C/D) from model output.
    
    Implements MMSI-Bench's extraction logic with word boundary checking.
    
    Args:
        text: Model output text
        
    Returns:
        Extracted letter or empty string if no valid answer found
    """
    if not text or not isinstance(text, str):
        return ""
    
    # Pattern 1: Extract content between double backticks ``...``
    pattern_1 = r'``([^`]*)``'
    match = re.search(pattern_1, text)
    if match:
        text = match.group(1)
    
    # Pattern 2: Extract content between single backticks `...`
    pattern_2 = r'`([^`]*)`'
    match = re.search(pattern_2, text)
    if match:
        text = match.group(1)
    
    # Pattern 3: Find isolated A/B/C/D with word boundary
    # This excludes cases like "A bike" (indefinite article)
    pattern_3 = r'\b[A-D]\b(?!\s[a-zA-Z])'
    match = re.search(pattern_3, text)
    if match:
        return match.group()
    
    return ""


def calculate_accuracy(predictions: List[str], ground_truths: List[str]) -> float:
    """Calculate overall accuracy.
    
    Args:
        predictions: List of predicted answers
        ground_truths: List of ground truth answers
        
    Returns:
        Accuracy as a float between 0 and 1
    """
    if not predictions or not ground_truths or len(predictions) != len(ground_truths):
        return 0.0
    
    correct = sum(
        pred.lower().strip() == gt.lower().strip()
        for pred, gt in zip(predictions, ground_truths)
    )
    return correct / len(predictions)


def get_mmsibench_metrics(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Calculate MMSI-Bench metrics including category-wise accuracy.
    
    Args:
        results: List of result dictionaries containing:
            - prediction: Model's predicted answer
            - answer: Ground truth answer
            - category: Task category
            
    Returns:
        Dictionary containing:
            - overall_accuracy: Overall accuracy
            - category_accuracy: Dict mapping category to accuracy
            - total_samples: Total number of samples
            - correct_samples: Number of correct predictions
            - category_counts: Dict mapping category to sample count
    """
    # Extract predictions and ground truths
    predictions = [r.get('prediction', '') for r in results]
    ground_truths = [r.get('answer', '') for r in results]
    
    # Calculate overall accuracy
    correct = 0
    total = len(results)
    
    for pred, gt in zip(predictions, ground_truths):
        if pred.lower().strip() == gt.lower().strip():
            correct += 1
    
    overall_acc = correct / total if total > 0 else 0.0
    
    # Calculate category-wise metrics
    category_correct = defaultdict(int)
    category_total = defaultdict(int)
    
    for result in results:
        category = result.get('category', 'unknown')
        pred = result.get('prediction', '')
        gt = result.get('answer', '')
        
        category_total[category] += 1
        if pred.lower().strip() == gt.lower().strip():
            category_correct[category] += 1
    
    category_accuracy = {
        cat: category_correct[cat] / category_total[cat]
        for cat in category_total
    }
    
    return {
        'overall_accuracy': overall_acc,
        'category_accuracy': category_accuracy,
        'total_samples': total,
        'correct_samples': correct,
        'category_counts': dict(category_total),
    }


# MMSI-Bench 11 task categories
MMSIBENCH_CATEGORIES = [
    "Camera-Camera",
    "Camera-Object", 
    "Camera-Region",
    "Object-Object",
    "Object-Region",
    "Region-Region",
    "Measurement",
    "Appearance",
    "Camera Motion",
    "Object Motion",
    "Multi-step Reasoning",
]
